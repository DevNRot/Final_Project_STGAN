{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Written By Devorah Rotman 316472026 and Carmit Kaye 346038169\n",
    "\n"
   ],
   "id": "7d0867babb022508"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "import os\n",
    "from scipy import linalg\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from STGAN.Data_loader_STGAN import get_content_loader, get_style_loader\n",
    "from STGAN.models_STGAN import Generator\n",
    "from torchvision.utils import save_image\n",
    "from skimage.exposure import match_histograms\n",
    "import warnings\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ],
   "id": "7137f52cb7585140",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T12:43:41.837938200Z",
     "start_time": "2025-07-02T12:42:08.449835Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def unnormalize_tanh(img_tensor):\n",
    "    return (img_tensor + 1) / 2\n",
    "\n",
    "def unnormalize_vgg(img):\n",
    "\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406], device=img.device).view(1,3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225], device=img.device).view(1,3, 1, 1)\n",
    "    return torch.clamp(img * std + mean, 0, 1)\n",
    "\n",
    "def tensor_to_numpy(img):\n",
    "    return img.permute(1, 2, 0).detach().cpu().numpy()\n",
    "\n",
    "def numpy_to_tensor(img_np):\n",
    "    return torch.tensor(img_np).permute(2, 0, 1).float()\n",
    "\n",
    "def gamma_correct(img, gamma=1.2):\n",
    "    return img.pow(gamma).clamp(0, 1)\n",
    "\n",
    "def match_to_content(generated_img, content_img):\n",
    "    \"\"\"\n",
    "    Match histogram of generated image to content image.\n",
    "    Both tensors should be [3, H, W], values in [0, 1]\n",
    "    \"\"\"\n",
    "    gen_np = generated_img.permute(1, 2, 0).detach().cpu().numpy()\n",
    "    con_np = content_img.permute(1, 2, 0).detach().cpu().numpy()\n",
    "\n",
    "    matched_np = match_histograms(gen_np, con_np, channel_axis=-1)\n",
    "    matched = torch.tensor(matched_np).permute(2, 0, 1).float().to(generated_img.device)\n",
    "    return matched.clamp(0, 1)\n",
    "\n",
    "\n",
    "class inference_and_metric():\n",
    "    def __init__(self, content_path, style_path, model_path, device, in_style, channels, batch_size, generated_directory_path):\n",
    "        self.content_loader = get_content_loader(content_path, batch_size)\n",
    "        self.style_loader = get_style_loader(style_path, batch_size)\n",
    "        self.model_path = model_path\n",
    "        self.device = device\n",
    "        #models - vgg, generator, feature extractor\n",
    "        self.gen = Generator(in_style, channels).to(device)\n",
    "        self.load_trained_model()\n",
    "        self.vgg = models.vgg19(pretrained=True).features.to(self.device).eval()\n",
    "        for param in self.vgg.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.feature_extractor =  models.inception_v3(pretrained=True, transform_input=False).to(device)\n",
    "        self.feature_extractor.fc = torch.nn.Identity()\n",
    "        self.save_dir = generated_directory_path\n",
    "\n",
    "\n",
    "    def load_trained_model(self):\n",
    "        ckpt = torch.load(self.model_path, map_location=self.device)\n",
    "        for i in range(ckpt['grow_rank']):\n",
    "            self.gen.grow()\n",
    "            self.gen.cuda()\n",
    "        self.gen.load_state_dict(ckpt['generator'])\n",
    "        for param in self.gen.parameters():\n",
    "            param.requires_grad = False\n",
    "        del ckpt\n",
    "\n",
    "    def get_activations_from_model(self):\n",
    "        \"\"\"Generate fake images from model and compute their Inception features.\"\"\"\n",
    "        self.vgg.eval()\n",
    "        self.gen.eval()\n",
    "        self.feature_extractor.eval()\n",
    "        activations = []\n",
    "        os.makedirs(self.save_dir, exist_ok=True) if self.save_dir is not None else None\n",
    "        counter = 0\n",
    "        transform = transforms.ColorJitter(contrast=0.5)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, _ in tqdm(self.content_loader):\n",
    "                inputs = inputs.to(self.device)\n",
    "\n",
    "                # Infer fake images using cGAN\n",
    "                feats = self.vgg(inputs)\n",
    "                generated = self.gen(feats)# assumes output shape [B, 3, H, W]\n",
    "\n",
    "                #possibly save the generated images\n",
    "                if self.save_dir is not None:\n",
    "                    for i, img in enumerate(generated):\n",
    "                        save_path = os.path.join(self.save_dir, f\"{counter:05}.png\")\n",
    "                        #orig = inputs[i]\n",
    "                        img = unnormalize_tanh(img)**1.8\n",
    "                        ##img = transform(img)\n",
    "                        #orig = unnormalize_vgg(orig)\n",
    "                        #matched = match_to_content(img, orig)\n",
    "                        #img = contrast_stretch(img)\n",
    "                        #img = gamma_correct(img)\n",
    "                        save_image(img.clamp( 0, 1), save_path)\n",
    "                        #save_image(matched.clamp(0, 1), save_path)\n",
    "                        #save_image(matched, save_path)\n",
    "                        counter += 1\n",
    "                #get features for FID\n",
    "                if generated.shape[-1] != 299:\n",
    "                    generated = torch.nn.functional.interpolate(generated, size=(299, 299), mode='bilinear', align_corners=False)\n",
    "\n",
    "                feats = self.feature_extractor(generated)\n",
    "                activations.append(feats.cpu().numpy())\n",
    "\n",
    "        return np.concatenate(activations, axis=0)\n",
    "\n",
    "    def get_activations_from_real(self):\n",
    "        \"\"\"Compute Inception features for real images (e.g., style images).\"\"\"\n",
    "        self.feature_extractor.eval()\n",
    "        activations = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, _ in tqdm(self.style_loader):\n",
    "                images = images.to(self.device)\n",
    "                if images.shape[-1] != 299:\n",
    "                    images = torch.nn.functional.interpolate(images, size=(299, 299), mode='bilinear', align_corners=False)\n",
    "\n",
    "                feats = self.feature_extractor(images)\n",
    "                activations.append(feats.cpu().numpy())\n",
    "\n",
    "        return np.concatenate(activations, axis=0)\n",
    "\n",
    "    def calculate_frechet_distance(self, mu1, sigma1, mu2, sigma2):\n",
    "        \"\"\"Standard FID (MiFID) computation.\"\"\"\n",
    "        diff = mu1 - mu2\n",
    "        covmean, _ = linalg.sqrtm(sigma1 @ sigma2, disp=False)\n",
    "        if np.iscomplexobj(covmean):\n",
    "            covmean = covmean.real\n",
    "        return diff.dot(diff) + np.trace(sigma1 + sigma2 - 2 * covmean)\n",
    "\n",
    "    def compute_mifid(self, generated_feats, real_feats):\n",
    "        \"\"\"\n",
    "        Compute Memorization-Informed FID (MiFID).\n",
    "        Args:\n",
    "            generated_feats (np.ndarray): shape (N_gen, 2048)\n",
    "            real_feats (np.ndarray): shape (N_real, 2048)\n",
    "        Returns:\n",
    "            float: MiFID score\n",
    "        \"\"\"\n",
    "        similarities = cosine_similarity(generated_feats, real_feats)  # Shape: [N_gen, N_real]\n",
    "        nearest_indices = np.argmax(similarities, axis=1)              # Most similar real image for each generated one\n",
    "        matched_real_feats = real_feats[nearest_indices]               # Shape: [N_gen, 2048]\n",
    "\n",
    "        mu_gen = np.mean(generated_feats, axis=0)\n",
    "        sigma_gen = np.cov(generated_feats, rowvar=False)\n",
    "\n",
    "        mu_real = np.mean(matched_real_feats, axis=0)\n",
    "        sigma_real = np.cov(matched_real_feats, rowvar=False)\n",
    "\n",
    "        return self.calculate_frechet_distance(mu_gen, sigma_gen, mu_real, sigma_real)\n",
    "\n",
    "    def compute_mifid_from_dataloaders(self):\n",
    "        # InceptionV3 up to pool3 (2048D)\n",
    "\n",
    "        print(\"Extracting features from generated images...\")\n",
    "        gen_acts = self.get_activations_from_model()\n",
    "        print(\"Extracting features from real style images...\")\n",
    "        style_acts = self.get_activations_from_real()\n",
    "\n",
    "\n",
    "        mifid_score = self.compute_mifid(gen_acts, style_acts)\n",
    "        print(\"MiFID score:\", mifid_score)\n",
    "        return mifid_score\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "7617b616c44b380a",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T12:43:41.841163400Z",
     "start_time": "2025-07-02T12:42:08.498797Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 32\n",
    "style_path = 'data/monet_jpg'\n",
    "content_path = 'data/photo_jpg'\n",
    "#model_path = 'STGAN/Samples/real_run_1/model.pt'\n",
    "model_path = 'STGAN/checkpoints/last.pt'\n",
    "in_style = 512 #size of w\n",
    "channels = [512, 512, 512, 256, 128, 64, 32] # layer channel sizes\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#generated_directory_path = 'STGAN/Samples/real_run_1/generated'\n",
    "generated_directory_path = None\n",
    "\n",
    "inferer = inference_and_metric(content_path, style_path, model_path, device, in_style, channels, batch_size, generated_directory_path)\n",
    "\n",
    "inferer.compute_mifid_from_dataloaders()"
   ],
   "id": "19eccaf81023a368",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from generated images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 219/219 [00:54<00:00,  3.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from real style images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:05<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MiFID score: 109.8110748614309\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(109.8110748614309)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
